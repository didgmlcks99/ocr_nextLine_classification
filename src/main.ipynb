{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 := delete \\n\n",
    "# 1 := leave as it is\n",
    "\n",
    "# model = gensim.models.Word2Vec.load('../../../pretrained_model/kor/ko.bin')\n",
    "\n",
    "# from hangul_utils import split_syllables, join_jamos\n",
    "# import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import timeit\n",
    "\n",
    "import data\n",
    "import bpe\n",
    "import utils\n",
    "import pretrained_model as pm\n",
    "import data_loader as dl\n",
    "import trainer\n",
    "import initializer as init\n",
    "import tester\n",
    "import model_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_np, second_np, label_np  = data.getData(0)\n",
    "\n",
    "no_first_np, no_second_np, no_label_np  = data.getData(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linearized complete!\n",
      "linearized complete!\n"
     ]
    }
   ],
   "source": [
    "first_np, second_np = utils.process_splitted(first_np, second_np)\n",
    "\n",
    "no_first_np, no_second_np = utils.process_splitted(no_first_np, no_second_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done tokenizing both data!\n",
      "saved ch2idx to file!\n",
      "encoding comlete!\n",
      "done tokenizing both data!\n",
      "saved ch2idx to file!\n",
      "encoding comlete!\n"
     ]
    }
   ],
   "source": [
    "first_ls, second_ls, ch2idx, max_len = utils.tokenize(first_np, second_np)\n",
    "first2idx_np, second2idx_np = utils.encode(first_ls, second_ls, ch2idx, max_len)\n",
    "\n",
    "no_first_ls, no_second_ls, no_ch2idx, no_max_len = utils.tokenize(no_first_np, no_second_np)\n",
    "no_first2idx_np, no_second2idx_np = utils.encode(no_first_ls, no_second_ls, no_ch2idx, no_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 / 133 pretrained vectors found.\n"
     ]
    }
   ],
   "source": [
    "# pretrained_word2vec = pm.load_pretrained_model(ch2idx)\n",
    "# pretrained_word2vec = torch.tensor(pretrained_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_first, test_first, train_second, test_second, train_labels, test_labels = train_test_split(\n",
    "    first2idx_np, second2idx_np, label_np, test_size = 0.1, random_state = 43\n",
    ")\n",
    "\n",
    "train_first, val_first, train_second, val_second, train_labels, val_labels = train_test_split(\n",
    "    train_first, train_second, train_labels, test_size = 0.1, random_state = 43\n",
    ")\n",
    "\n",
    "no_train_first, no_test_first, no_train_second, no_test_second, no_train_labels, no_test_labels = train_test_split(\n",
    "    no_first2idx_np, no_second2idx_np, no_label_np, test_size = 0.1, random_state = 43\n",
    ")\n",
    "\n",
    "no_train_first, no_val_first, no_train_second, no_val_second, no_train_labels, no_val_labels = train_test_split(\n",
    "    no_train_first, no_train_second, no_train_labels, test_size = 0.1, random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = dl.data_loader(\n",
    "                                                                   train_first,\n",
    "                                                                   train_second,\n",
    "                                                                   val_first,\n",
    "                                                                   val_second,\n",
    "                                                                   test_first,\n",
    "                                                                   test_second,\n",
    "                                                                   train_labels,\n",
    "                                                                   val_labels,\n",
    "                                                                   test_labels,\n",
    "                                                                   batch_size=100)\n",
    "\n",
    "no_train_dataloader, no_val_dataloader, no_test_dataloader = dl.data_loader(\n",
    "                                                                   no_train_first,\n",
    "                                                                   no_train_second,\n",
    "                                                                   no_val_first,\n",
    "                                                                   no_val_second,\n",
    "                                                                   no_test_first,\n",
    "                                                                   no_test_second,\n",
    "                                                                   no_train_labels,\n",
    "                                                                   no_val_labels,\n",
    "                                                                   no_test_labels,\n",
    "                                                                   batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TensorBoard support\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('../tensorboard/final/split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================\n",
    "# set parameters here\n",
    "# ====================\n",
    "\n",
    "title = 'split_test_1'\n",
    "epochs = 40\n",
    "\n",
    "vocab_size=len(ch2idx)\n",
    "embed_dim = 100\n",
    "hidden_size = 200\n",
    "num_classes = 2\n",
    "rnn_layers = 1\n",
    "\n",
    "num_filters = [100, 200, 100]\n",
    "kernel_sizes = [15, 21, 114]\n",
    "\n",
    "dropout = 0.2\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model_name=\"RNN\"\n",
    "optim_name=\"Adam\"\n",
    "loss_fn_name=\"CEL\"\n",
    "\n",
    "pretrained_model=None\n",
    "freeze_embedding=False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing with pretrained model!!!\n",
      "OCR_rnn(\n",
      "  (emb): Embedding(133, 200)\n",
      "  (lstm1): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (lstm2): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (dp1): Dropout(p=0.0, inplace=False)\n",
      "  (dp2): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer.set_seed(42)\n",
    "\n",
    "model, optimizer, loss_fn = init.initialize_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes,\n",
    "    rnn_layers=rnn_layers,\n",
    "    num_filters=num_filters,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout=dropout,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    model_name=model_name,\n",
    "    optim_name=optim_name,\n",
    "    loss_fn_name=loss_fn_name,\n",
    "    pretrained_model=pretrained_model,\n",
    "    freeze_embedding=freeze_embedding,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed\n",
      "--------------------------------------------------------------------------------\n",
      "   1    |   0.236924   | 91.004115  | 0.163169 | 94.70  | 70.05 \n",
      "   2    |   0.124648   | 95.481481  | 0.116488 | 96.11  | 68.64 \n",
      "   3    |   0.097195   | 96.506173  | 0.098676 | 96.85  | 68.49 \n",
      "   4    |   0.076764   | 97.288066  | 0.081948 | 97.00  | 68.74 \n",
      "   5    |   0.066478   | 97.514403  | 0.086427 | 97.00  | 68.47 \n",
      "   6    |   0.058285   | 97.946502  | 0.085777 | 96.70  | 70.24 \n",
      "   7    |   0.051059   | 98.078189  | 0.075075 | 97.33  | 69.58 \n",
      "   8    |   0.043834   | 98.403292  | 0.077979 | 97.56  | 72.06 \n",
      "   9    |   0.041250   | 98.518519  | 0.087841 | 97.19  | 72.10 \n",
      "  10    |   0.032298   | 98.843621  | 0.073635 | 97.74  | 71.92 \n",
      "  11    |   0.028712   | 98.938272  | 0.090851 | 97.15  | 71.74 \n",
      "  12    |   0.024686   | 99.061728  | 0.101292 | 97.15  | 72.77 \n",
      "  13    |   0.017271   | 99.395062  | 0.112292 | 97.19  | 72.49 \n",
      "  14    |   0.024741   | 99.049383  | 0.097130 | 97.37  | 72.01 \n",
      "  15    |   0.012779   | 99.551440  | 0.107819 | 97.30  | 70.66 \n",
      "  16    |   0.010282   | 99.646091  | 0.135683 | 97.04  | 70.48 \n",
      "  17    |   0.016459   | 99.419753  | 0.115991 | 97.26  | 70.64 \n",
      "  18    |   0.012153   | 99.559671  | 0.125401 | 97.48  | 70.73 \n",
      "  19    |   0.012645   | 99.534979  | 0.116011 | 97.37  | 70.51 \n",
      "  20    |   0.011723   | 99.572016  | 0.128530 | 97.26  | 70.30 \n",
      "  21    |   0.006934   | 99.757202  | 0.121225 | 97.19  | 70.59 \n",
      "  22    |   0.008158   | 99.732510  | 0.132007 | 97.59  | 70.67 \n",
      "  23    |   0.007292   | 99.740741  | 0.134740 | 97.26  | 70.60 \n",
      "  24    |   0.007782   | 99.748971  | 0.136647 | 97.52  | 70.60 \n",
      "  25    |   0.008990   | 99.662551  | 0.125710 | 97.22  | 70.39 \n",
      "  26    |   0.010335   | 99.633745  | 0.129695 | 97.33  | 70.44 \n",
      "  27    |   0.008638   | 99.711934  | 0.141759 | 97.22  | 70.37 \n",
      "  28    |   0.005942   | 99.810700  | 0.133647 | 97.30  | 71.18 \n",
      "  29    |   0.004557   | 99.851852  | 0.145495 | 97.59  | 71.77 \n",
      "  30    |   0.003133   | 99.888889  | 0.166141 | 97.33  | 68.60 \n",
      "  31    |   0.008321   | 99.711934  | 0.154004 | 97.63  | 69.35 \n",
      "  32    |   0.011274   | 99.646091  | 0.140245 | 97.44  | 70.99 \n",
      "  33    |   0.001975   | 99.934156  | 0.155083 | 97.44  | 71.33 \n",
      "  34    |   0.000970   | 99.971193  | 0.171493 | 97.52  | 69.12 \n",
      "  35    |   0.000321   | 99.995885  | 0.190824 | 97.41  | 69.33 \n",
      "  36    |   0.000155   | 100.000000 | 0.191516 | 97.52  | 68.60 \n",
      "  37    |   0.000057   | 100.000000 | 0.195414 | 97.56  | 68.43 \n",
      "  38    |   0.000032   | 100.000000 | 0.201332 | 97.56  | 69.08 \n",
      "  39    |   0.000024   | 100.000000 | 0.205232 | 97.59  | 68.60 \n",
      "  40    |   0.000018   | 100.000000 | 0.209742 | 97.67  | 67.91 \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 97.74%.\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    epochs=epochs,\n",
    "    title=title+'yes_gudu',\n",
    "    writer=writer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    device=device,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.saveModel(title+'yes_gudu', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR_rnn(\n",
      "  (emb): Embedding(133, 200)\n",
      "  (lstm1): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (lstm2): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (dp1): Dropout(p=0.0, inplace=False)\n",
      "  (dp2): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = mu.getModel(title+'yes_gudu')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  3.0156676789124806\n",
      "test acc:  80.2\n",
      "saved precision and recall results to file!\n"
     ]
    }
   ],
   "source": [
    "loss, acc = tester.test(test_dataloader=no_test_dataloader,\n",
    "                        device=device,\n",
    "                        model=model,\n",
    "                        title=title+'yes_gudu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../result/final_split', 'a') as f:\n",
    "        text = title + '\\t |\\tloss: ' + str(loss) + '\\t |\\tacc: ' + str(acc) + '\\t |\\t time: ' + str(round(end_time, 3)) + ' min\\n'\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded model graph to tensorboard!\n"
     ]
    }
   ],
   "source": [
    "mu.graphModel(train_dataloader, model, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing with pretrained model!!!\n",
      "OCR_rnn(\n",
      "  (emb): Embedding(133, 200)\n",
      "  (lstm1): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (lstm2): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (dp1): Dropout(p=0.0, inplace=False)\n",
      "  (dp2): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainer.set_seed(42)\n",
    "\n",
    "no_model, no_optimizer, no_loss_fn = init.initialize_model(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes,\n",
    "    rnn_layers=rnn_layers,\n",
    "    num_filters=num_filters,\n",
    "    kernel_sizes=kernel_sizes,\n",
    "    dropout=dropout,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    model_name=model_name,\n",
    "    optim_name=optim_name,\n",
    "    loss_fn_name=loss_fn_name,\n",
    "    pretrained_model=pretrained_model,\n",
    "    freeze_embedding=freeze_embedding,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(no_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed\n",
      "--------------------------------------------------------------------------------\n",
      "   1    |   0.312176   | 87.419753  | 0.225479 | 91.56  | 68.37 \n",
      "   2    |   0.188457   | 93.004115  | 0.179643 | 93.19  | 68.51 \n",
      "   3    |   0.139370   | 94.798354  | 0.134548 | 94.96  | 68.48 \n",
      "   4    |   0.115777   | 95.584362  | 0.120019 | 95.37  | 68.42 \n",
      "   5    |   0.097988   | 96.316872  | 0.137769 | 94.74  | 68.16 \n",
      "   6    |   0.082520   | 96.934156  | 0.117215 | 96.22  | 68.51 \n",
      "   7    |   0.068715   | 97.415638  | 0.100947 | 96.30  | 69.12 \n",
      "   8    |   0.059495   | 97.777778  | 0.107078 | 96.07  | 69.24 \n",
      "   9    |   0.051698   | 98.123457  | 0.127317 | 95.78  | 68.92 \n",
      "  10    |   0.046423   | 98.205761  | 0.118974 | 96.19  | 68.44 \n",
      "  11    |   0.036776   | 98.658436  | 0.110807 | 96.41  | 68.02 \n",
      "  12    |   0.033592   | 98.860082  | 0.125282 | 95.93  | 68.02 \n",
      "  13    |   0.028101   | 98.987654  | 0.143943 | 96.04  | 68.57 \n",
      "  14    |   0.026716   | 99.016461  | 0.126506 | 96.63  | 68.85 \n",
      "  15    |   0.024529   | 99.148148  | 0.141876 | 96.22  | 68.73 \n",
      "  16    |   0.017006   | 99.415638  | 0.147827 | 96.48  | 68.55 \n",
      "  17    |   0.018858   | 99.292181  | 0.172574 | 96.19  | 68.80 \n",
      "  18    |   0.018582   | 99.345679  | 0.164039 | 96.67  | 68.73 \n",
      "  19    |   0.015561   | 99.427984  | 0.150895 | 96.59  | 68.68 \n",
      "  20    |   0.008796   | 99.744856  | 0.147313 | 96.48  | 68.11 \n",
      "  21    |   0.005217   | 99.868313  | 0.184367 | 96.37  | 68.62 \n",
      "  22    |   0.021369   | 99.213992  | 0.156518 | 96.44  | 68.78 \n",
      "  23    |   0.011032   | 99.555556  | 0.203924 | 96.22  | 68.84 \n",
      "  24    |   0.005743   | 99.798354  | 0.211440 | 96.41  | 68.97 \n",
      "  25    |   0.005786   | 99.810700  | 0.219114 | 96.19  | 68.66 \n",
      "  26    |   0.018319   | 99.312757  | 0.182067 | 96.41  | 68.80 \n",
      "  27    |   0.008448   | 99.699588  | 0.208940 | 96.11  | 68.59 \n",
      "  28    |   0.008160   | 99.695473  | 0.198835 | 96.26  | 68.76 \n",
      "  29    |   0.012140   | 99.600823  | 0.179223 | 96.52  | 68.28 \n",
      "  30    |   0.003599   | 99.888889  | 0.220629 | 96.48  | 68.09 \n",
      "  31    |   0.009266   | 99.641975  | 0.188954 | 96.26  | 68.09 \n",
      "  32    |   0.009499   | 99.707819  | 0.192899 | 96.26  | 68.29 \n",
      "  33    |   0.009266   | 99.674897  | 0.205501 | 96.48  | 68.63 \n",
      "  34    |   0.003025   | 99.901235  | 0.238518 | 96.00  | 68.70 \n",
      "  35    |   0.010793   | 99.572016  | 0.214843 | 96.15  | 68.82 \n",
      "  36    |   0.005245   | 99.843621  | 0.240172 | 96.52  | 69.11 \n",
      "  37    |   0.006368   | 99.781893  | 0.221774 | 96.11  | 68.42 \n",
      "  38    |   0.008525   | 99.679012  | 0.200966 | 96.70  | 68.90 \n",
      "  39    |   0.006728   | 99.773663  | 0.227466 | 96.41  | 68.44 \n",
      "  40    |   0.002131   | 99.913580  | 0.251585 | 96.41  | 68.57 \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 96.70%.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    epochs=epochs,\n",
    "    title=title+'no_gudu',\n",
    "    writer=writer,\n",
    "    train_dataloader=no_train_dataloader,\n",
    "    val_dataloader=no_val_dataloader,\n",
    "    device=device,\n",
    "    model=no_model,\n",
    "    optimizer=no_optimizer,\n",
    "    loss_fn=no_loss_fn\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.saveModel(title+'no_gudu', no_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR_rnn(\n",
      "  (emb): Embedding(133, 200)\n",
      "  (lstm1): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (lstm2): RNN(\n",
      "    (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (fc1): Linear(in_features=400, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=2, bias=True)\n",
      "  (dp1): Dropout(p=0.0, inplace=False)\n",
      "  (dp2): Dropout(p=0.0, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_model = mu.getModel(title+'no_gudu')\n",
    "print(no_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  0.20598777152918046\n",
      "test acc:  97.36666666666667\n",
      "saved precision and recall results to file!\n"
     ]
    }
   ],
   "source": [
    "loss, acc = tester.test(test_dataloader=no_test_dataloader,\n",
    "                        device=device,\n",
    "                        model=no_model,\n",
    "                        title=title+'no_gudu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../result/final_split', 'a') as f:\n",
    "        text = title + '\\t |\\tloss: ' + str(loss) + '\\t |\\tacc: ' + str(acc) + '\\t |\\t time: ' + str(round(end_time, 3)) + ' min\\n'\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded model graph to tensorboard!\n"
     ]
    }
   ],
   "source": [
    "mu.graphModel(no_train_dataloader, no_model, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "348b9cd948ce87438be2e622031b2ecfa29bc2d3ecc0fd03127b9a24b30227df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
